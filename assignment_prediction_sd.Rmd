---
title: "Assignment Prediction"
author: "Ruben van den Goorbergh, ZoÃ« Dunias & Thom Volker"
date: "1/18/2021"
output: html_document
---

# Assignment prediction model

## A model to retain credit card service customers

### Goal and data set

The BankChurners data set contains information on ~10,000 customers of a bank regarding the use of a credit card service. The goal here is to predict whether a customer will churn or not so that the bank can take action before the customer actually does quit the service. 

The data set consists of 23 variables, both categorical and numerical. The last two variables are results or previous classifiers and will therefore be omitted. The variable indicating whether a customer is still active is called 'Attrition_Flag', this will be our dependent variable. A description of the variables as well as the data set can be found [here](https://www.kaggle.com/sakshigoyal7/credit-card-customers). <!-- Is dat niet onze taak, om de variabelen die we gebruiken ook te beschrijven? -->

#### Packages

```{r, message = FALSE}
library(tidyverse)
library(glmnet)
library(randomForest)
library(caret)
```

#### Reading in the data

```{r}
set.seed(123)
dat <-read.csv("BankChurners.csv")
dat <- dat[,- c(22:23)]
dat$Attrition_Flag <-factor(dat$Attrition_Flag)
```

### Quick EDA

To get an idea of what the data looks like, we first performed a quick EDA.

The first thing to notice is that we are dealing with a data set that is quite imbalanced. We have to keep this in mind when we choose our model. If we go for a model that is easily affected by unevenly distributed outcomes (e.g. a tree based model), we might want to do something about the imbalance before fitting the model.

```{r}
ggplot(data = dat, aes(x = Attrition_Flag)) +
  geom_bar()
```


To explore the distribution of the predictors per outcome class we made a series of plots. In order to compare them the y-scale is standardized.

```{r, fig.height=10, fig.width=10}
bar_plots <- dat %>%
  select(!where(is.numeric)) %>%
  pivot_longer(-Attrition_Flag) %>%
  ggplot(aes(x = value, fill = Attrition_Flag)) +
  geom_bar(position = "fill") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  facet_wrap(~name, ncol = 2, scales = "free")

bar_plots

num_plots <- dat %>%
  select(Attrition_Flag, where(is.numeric)) %>%
  pivot_longer(-Attrition_Flag) %>%
  ggplot(aes(x = value, fill = Attrition_Flag)) +
  geom_density(alpha = 0.6) +
  facet_wrap(~name, ncol = 2, scales = "free")

num_plots
```

Based on the distribution of the variables per outcome class, potential interesting variables to consider are:
<!-- Dit is allemaal univariate, dus kan zijn dat dit anders is na het controleren voor andere variabelen -->

* Card category
* Total_Relationship_Count
* Months_Inactive_12_mon
* Contacts_Count_12_mon
* Total_Revolving_Bal
* Total_Amt_Chng_Q4_Q1
* Total_Trans_Amt
* Total_Trans_Ct
* Total_Ct_Chng_Q4_Q1
* Avg_Utilization_Ratio

### Model fitting

Before we start fitting models, we first split up the data into a test and a training part to make sure we can make a fair estimation of the performance of each model.

```{r}
train_index <- sample(seq(1, nrow(dat), 1), round(0.8*nrow(dat)))
dat_train <- dat[train_index,]
dat_test <- dat[-train_index,]
```

For this task, we will try three different methods: regular logistic regression, L1 logistic regression and a random forest. The model that performs the best will be further evaluated and interpreted.

```{r}
# Regular logistic regression
lr_model <- glm(Attrition_Flag ~., family = binomial, data = dat_train) 
```

```{r}
# L1 logistic regression

# create model matrix to train model
x_train <- model.matrix(Attrition_Flag ~., data = dat_train)

# Tune lambda using 10-fold cv
lambda <- cv.glmnet(x = x_train[,-1],
                      y = dat_train$Attrition_Flag,
                      family = 'binomial',
                      alpha = 1)
                      
# Train model with lowest lambda
lr_l1_model <- glmnet(x = x_train[,-1],
                      y = dat_train$Attrition_Flag,
                      family = 'binomial',
                      alpha = 1,
                      lambda = lambda$lambda.min)

lambda$lambda.min
```

It can be seen that the best lambda is very close to zero, meaning that the model gets hardly penalized for using extra parameters. This isn't surprising since the data set is quite large and hence the model is less prone to overfitting of the data.


```{r}
# Fit random forest model
rf_model <- randomForest(Attrition_Flag ~., data = dat_train)
```

Now all models are fitted, we can use them to make predictions on the test data. Unlike random forest, logistic regression models estimate probabilities rather than outcomes. Hence we used a cut off value of .5 to determine whether a customer was predicted to have an active account or not.

```{r}
x_test <- model.matrix(Attrition_Flag ~.,data = dat_test)
pred <- dat_test$Attrition_Flag

lr_prob <- predict(lr_model, newdata = dat_test, type = 'response')
lr_pred <- factor(ifelse(lr_prob > 0.5, 2, 1))
levels(lr_pred) <- c("Attrited Customer", "Existing Customer")

lr_l1_prob <- predict(lr_l1_model, newx = x_test[,-1], type = 'response')
lr_l1_pred <- factor(ifelse(lr_l1_prob > 0.5, 2, 1))
levels(lr_l1_pred) <- c("Attrited Customer", "Existing Customer")

rf_pred <-  predict(rf_model, newdata = dat_test)
```

Having all prediction, we can create confusion matrices to compare the performance of the different models.

```{r}
confusionMatrix(reference = pred, data = lr_pred)
confusionMatrix(reference = pred, data = lr_l1_pred)
confusionMatrix(reference = pred, data = rf_pred)
```

It can be seen that the random forest model yield the best results in both recognizing the positive and negative cases (sensitivity and specificity). We specifically used those performance metrics instead of using accuracy because accuracy is strongly influenced by the performance of the model regarding the majority class.

To see whether we can improve the model a little more we will tune some of its parameters and use a simple under sampling technique to deal with the imbalanced data set.

Because imbalanced data can lead to a bias in the majority class, we want to balance this out.
The data set is sufficiently large and hence we chose to use random undersampling to 
deal with the imbalance

```{r}
dat_us <- caret::downSample(x = dat_train[, -2], 
                            y = dat_train$Attrition_Flag,
                            yname = "Attrition_Flag")
```

```{r}
rf_us_model <- randomForest(Attrition_Flag~., data = dat_us)
rf_us_pred <- predict(rf_us_model, newdata = dat_test)
```

```{r}
confusionMatrix(data = rf_us_pred, reference = pred)

```
We can see that the sensitivity of the model vastly improves when using undersampling
at the cost of some specificity. A way to display the trade-off between sensitivity
and specificity in one meric is the f1 score 

$$f1 score = 2*((precision*recall)/(precision+recall))$$

As can be seen from the table below, the f1 score improves using undersampling.

```{r}
f1 <- 2*((0.99*0.8)/(0.99+0.8))
f1_undersampling <- 2*((0.94*0.95)/(0.94+0.95))

tibble(f1, f1_undersampling)
```

Now we tune the mtry parameter using the tuneRF function from the 'randomForest' package.
```{r}
# Algorithm Tune (tuneRF)
set.seed(123)
bestmtry <- tuneRF(dat_us[, c(1, 3:21)], dat_us[,2], stepFactor= 1.3, improve=0.001, ntree=500)
print(bestmtry)
```
It can be seen from the plot that the best Out Of Bag error is achieved at an mtry 
value of 14. The plot nicely illustrates the bias-variance trade off as the OOB error
declines at first (decreasing bias, slowly increasing variance) and then after it
reaches its optimum the decreasing bias doesn't make up for the increasing variance
anymore and hence leads to a higher OOB error.

```{r}
rf_us_model_2 <- randomForest(Attrition_Flag~., data = dat_us, mtry = 14)
rf_us_pred_2 <- predict(rf_us_model, newdata = dat_test)
```

```{r}
confusionMatrix(data = rf_us_pred_2, reference = pred)

```

The tuning of mtry doesn't lead to any notable differences in performance in our case.


### Conclusions

TO assess the importance of the variables in our final model, we use the function 
imporance() from the 'randomForest' package.
```{r}
importance(rf_us_model_2)
```

