---
title: "Assignment Prediction"
author:
- Ruben van den Goorbergh, ZoÃ« Dunias, Cassandra Bunschoten, Paulina von Stackelberg, Sofie van den Brand and Thom Volker
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
```

# A model to retain credit card service customers

# Goal and data set

The BankChurners data set contains information on ~10,000 customers of a bank regarding the use of a credit card service. The goal here is to predict whether a customer will churn or not so that the bank can take action before the customer actually does quit the service. 

The data set consists of 23 variables, both categorical and numerical. The first variable is a person identifier, so it will not be used in any of the analyses and the last two variables are results or previous classifiers and will therefore be omitted. The variable indicating whether a customer is still active is called 'Attrition_Flag' (Existing Customer or Attrited Customer), this will be our dependent variable. The data set can be found [here](https://www.kaggle.com/sakshigoyal7/credit-card-customers). 

The data consists of the following variables:

- CLIENTNUM: Client number: unique identifier for the customer holding the account.
- Customer_Age: Customer's age in years.
- Gender: Customer's gender (M=Male, F=Female).
- Dependent_count: Number of dependents.
- Education_Level: Educational Qualification of the account holder.
- Marital_Status: Married, Single, Divorced or unknown.
- Income_Category: annual Income Category of the account holder ($\$0 < \$40K, \$40K - \$60K, \$60K - \$80K, \$80K - \$120K, > \$120K$).
- Card_Category: type of Card (Blue, Silver, Gold, Platinum).
- Months_on_book: period of relationship with bank.
- Total_Relationship_Count: total no. of products held by the customer.
- Months_Inactive_12_mon: No. of Contacts in the last 12 months.
- Credit_Limit: credit limit on the credit card.
- Total_Revolving_Bal: total revolving balance on the credit card
- Avg_Open_To_Buy: open to buy credit line (average of last 12 months).
- Total_Amt_Chng_Q4_Q1: change in transaction amount (Q4 over Q1).
- Total_Trans_Amt: total transaction amount (Last 12 months).
- Total_Trans_Ct: total transaction count (Last 12 months).
- Total_Ct_Chng_Q4_Q1: change in transaction count (Q4 over Q1).
- Avg_Utilization_Ratio: average card utilization ratio.


# Packages

```{r, message = FALSE}
library(tidyverse)
library(glmnet)
library(randomForest)
library(caret)
library(kableExtra)
```

# Reading in the data

```{r}
set.seed(123)
dat <- read.csv("BankChurners.csv")
dat <- dat[, -c(1, 22:23)]
dat$Attrition_Flag <- factor(dat$Attrition_Flag)
```

# Quick EDA

To get an idea of what the data looks like, we first performed a quick EDA.

The first thing to notice is that we are dealing with a data set that is quite imbalanced. We have to keep this in mind when we choose our model. If we go for a model that is easily affected by unevenly distributed outcomes (e.g. a tree based model), we might want to do something about the imbalance before fitting the model.

```{r}
dat %>% 
  select(where(is.numeric)) %>% 
  psych::describe() %>% 
  select(n, mean, sd, median, min, max) %>%
  kable(digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
  
dat %>% 
  select(!where(is.numeric)) %>% 
  map(function(x) table(x))

ggplot(data = dat, aes(x = Attrition_Flag)) +
  geom_bar()
```


The bivariate relationships between the predictor variables and the outcome variable `Attrition_Flag` is displayed below, in density plots for the numeric predictors and in barplots for the categorical predictors. 

```{r, fig.height=10, fig.width=10}
bar_plots <- dat %>%
  select(!where(is.numeric)) %>%
  pivot_longer(-Attrition_Flag) %>%
  ggplot(aes(x = value, fill = Attrition_Flag)) +
  geom_bar(position = "fill") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  facet_wrap(~name, ncol = 2, scales = "free")

bar_plots

num_plots <- dat %>%
  select(Attrition_Flag, where(is.numeric)) %>%
  pivot_longer(-Attrition_Flag) %>%
  ggplot(aes(x = value, fill = Attrition_Flag)) +
  geom_density(alpha = 0.6) +
  facet_wrap(~name, ncol = 2, scales = "free")

num_plots
```

Based on the distribution of the variables per outcome class, potential interesting variables to consider are:

* Card category
* Total_Relationship_Count
* Months_Inactive_12_mon
* Contacts_Count_12_mon
* Total_Revolving_Bal
* Total_Amt_Chng_Q4_Q1
* Total_Trans_Amt
* Total_Trans_Ct
* Total_Ct_Chng_Q4_Q1
* Avg_Utilization_Ratio

Note that this concerns univariate relationships, and assessing multivariate associations might reveal a different picture.

# Model fitting

Before we start fitting models, we first split up the data into a test ($20\%$ of the data) and a training ($80\%$ of the data) part to make sure we can make a fair estimation of the performance of each model.

```{r}
train_index <- sample(seq(1, nrow(dat), 1), round(0.8 * nrow(dat)))
dat_train <- dat[train_index,]
dat_test <- dat[-train_index,]
```

For the current assignment, we will try three different methods that are all able to predict the dichotomous outcome `Attrition_Flag`: regular logistic regression, L1 regularized logistic regression and a random forest. The model that performs best will be further evaluated and interpreted.

```{r}
# Regular logistic regression
lr_model <- glm(Attrition_Flag ~ ., family = binomial, data = dat_train) 
```

```{r}
# L1 logistic regression
# create model matrix to train model
x_train <- model.matrix(Attrition_Flag ~ ., data = dat_train)

# Tune lambda using 10-fold cv
lambda <- cv.glmnet(x = x_train[, -1],
                      y = dat_train$Attrition_Flag,
                      family = 'binomial',
                      alpha = 1)
                      
# Train model with lowest lambda
lr_l1_model <- glmnet(x = x_train[, -1],
                      y = dat_train$Attrition_Flag,
                      family = 'binomial',
                      alpha = 1,
                      lambda = lambda$lambda.min)

lambda$lambda.min
```

It can be seen that the best lambda is very close to zero, meaning that the model gets hardly penalized for using extra parameters. This isn't surprising since the data set is quite large and hence the model is less prone to overfitting of the data.


```{r}
# Fit random forest model
rf_model <- randomForest(Attrition_Flag ~ ., data = dat_train)
```

Now all models are fitted, we can use them to make predictions on the test data. Unlike random forest, logistic regression models estimate probabilities rather than outcomes. Hence we used a cut off value of .5 to determine whether a customer was predicted to have an active account or not.

```{r}
x_test <- model.matrix(Attrition_Flag ~ ., data = dat_test)
pred <- dat_test$Attrition_Flag

lr_prob <- predict(lr_model, newdata = dat_test, type = 'response')
lr_pred <- factor(ifelse(lr_prob > 0.5, 2, 1))
levels(lr_pred) <- c("Attrited Customer", "Existing Customer")

lr_l1_prob <- predict(lr_l1_model, newx = x_test[, -1], type = 'response')
lr_l1_pred <- factor(ifelse(lr_l1_prob > 0.5, 2, 1))
levels(lr_l1_pred) <- c("Attrited Customer", "Existing Customer")

rf_pred <-  predict(rf_model, newdata = dat_test)
```

Having all predictions, we can create confusion matrices to compare the performance of the different models.

```{r}
confusionMatrix(reference = pred, data = lr_pred)
confusionMatrix(reference = pred, data = lr_l1_pred)
confusionMatrix(reference = pred, data = rf_pred)
```

It can be seen that the random forest model yields the best results in both recognizing the positive and negative cases (sensitivity and specificity). We specifically used those performance metrics instead of using accuracy because accuracy is strongly influenced by the performance of the model regarding the majority class.

To see whether we can improve the model a little more we will tune some of its parameters and use a simple under sampling technique to deal with the imbalanced data set.

Because imbalanced data can lead to a bias in the majority class, we want to balance this out. The data set is sufficiently large and hence we chose to use random undersampling to deal with the imbalance.

```{r}
dat_us <- caret::downSample(x = dat_train[, -1], 
                            y = dat_train$Attrition_Flag,
                            yname = "Attrition_Flag")
```

```{r}
rf_us_model <- randomForest(Attrition_Flag ~ ., data = dat_us)
rf_us_pred <- predict(rf_us_model, newdata = dat_test)
```

```{r}
confusionMatrix(data = rf_us_pred, reference = pred)
```
We can see that the sensitivity of the model vastly improves when using undersampling at the cost of some specificity. A way to display the trade-off between sensitivity and specificity in one metric is the balanced accuracy 

$$\text{balanced accuracy} = \frac{sensitivity + specificity}{2}$$

As can be seen from the table below, the balanced accuracy improves using undersampling.

```{r}
bal_acc <- sum(confusionMatrix(reference = pred, data = rf_pred)$byClass[c(1,2)])/2
bal_acc_undersampling <- sum(confusionMatrix(data = rf_us_pred, reference = pred)$byClass[c(1,2)])/2

tibble(bal_acc, bal_acc_undersampling) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

Possibly, we can improve the model even further, by tuning the `mtry` parameter using the `tuneRF` function from the `randomForest` package.

```{r}
# Algorithm Tune (tuneRF)
bestmtry <- tuneRF(dat_us[, c(2:20)], 
                   dat_us[, 1], 
                   stepFactor = 1.3, 
                   improve = 0.001, 
                   ntree = 500)
print(bestmtry)
```

It can be seen from the plot that the best Out Of Bag (OOB) error is achieved at an `mtry` value of 7. The plot nicely illustrates the bias-variance trade off as the OOB error declines at first (decreasing bias, slowly increasing variance) and then after it reaches its optimum the decreasing bias doesn't make up for the increasing variance anymore and hence leads to a higher OOB error.

```{r}
rf_us_model_2 <- randomForest(Attrition_Flag ~ ., data = dat_us, mtry = 7)
rf_us_pred_2 <- predict(rf_us_model_2, newdata = dat_test)
```

```{r}
final_conf_mat <- confusionMatrix(data = rf_us_pred_2, reference = pred)
final_conf_mat
```

The tuning of `mtry` does not lead to any notable differences in performance in our case.


# Conclusions

To assess the importance of the variables in our final model, we use the function `importance()` from the 'randomForest' package.

```{r}
imp_final <- importance(rf_us_model_2)

imp_final %>%
  kable(digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

It can be seen that the variable `Total_Trans_Ct`, the total transaction count in the last twelve months, is the most influential variable in our model, as including it results in a decrease in the GINI index of `r round(imp_final[which.max(imp_final)], 2)`. The second most important variable in the model is `Total_Trans_Amt`, which represents the total transaction amount in the last twelve months, with a GINI decrease of `r round(sort(imp_final)[18], 2)`. The third most important variable is the variable `Total_Revolving_Bal`, reflecting the total revolving balance on the customer's credit card, with a GINI decrease of `r round(sort(imp_final)[17], 2)`. As such, these variables are most important in predicting whether or not an account is closed. Bivariately, a higher score on these variables is related to a higher probability of being an active customer. However, due to the nature of the analysis method (random forests), it is not possible to further assess the relationship between these variables and our outcome. Nevertheless, since this model performs so much better than the alternative models considered, we take the difficulties with interpreting the effects of individual variables for granted, as the model allows us to make the best predictions regarding whether or not a person becomes an attrited customer. Namely, the final model has a sensitivity of `r round(final_conf_mat$byClass[1], 2)` and a specificity of `r round(final_conf_mat$byClass[2], 2)`, resulting in a balanced accuracy of `r round(final_conf_mat$byClass[11], 2)`. Thus, in around $94\%$ of all cases, our model is capable of correctly classifying those who attrited and those who remained active customers. Then, it is up to the management team of the respective bank whether this is good enough, or whether it is more important to detect the potentially attriting customers, in which case an even higher sensitivity might be aimed for. 

